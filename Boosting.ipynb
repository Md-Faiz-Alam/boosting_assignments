{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "#Theoretical Answers:-\n",
        "---\n",
        "---\n",
        "###1. What is Boosting in Machine Learning?\n",
        "Boosting is an ensemble technique that combines multiple weak learners (typically decision trees) sequentially to create a strong learner. Each new model corrects the errors of the previous ones by focusing more on the incorrectly predicted instances.\n",
        "\n",
        "---\n",
        "###2. How does Boosting differ from Bagging?\n",
        "| Feature        | **Bagging**                | **Boosting**                          |\n",
        "| -------------- | -------------------------- | ------------------------------------- |\n",
        "| Model Training | Parallel                   | Sequential                            |\n",
        "| Focus          | Reduces variance           | Reduces bias                          |\n",
        "| Weights        | Equal weight to all models | Adjusted weights based on performance |\n",
        "| Example        | Random Forest              | AdaBoost, Gradient Boosting, XGBoost  |\n",
        "\n",
        "---\n",
        "###3. What is the key idea behind AdaBoost?\n",
        "The key idea of AdaBoost (Adaptive Boosting) is to:\n",
        "\n",
        "- Train weak learners sequentially.\n",
        "\n",
        "- Increase the weight of misclassified samples so that the next learner focuses more on them.\n",
        "\n",
        "- Combine all learners using a weighted majority vote (for classification) or weighted sum (for regression).\n",
        "\n",
        "---\n",
        "###4. Explain the working of AdaBoost with an example:\n",
        "Example:\n",
        "\n",
        "- Dataset: Binary classification (Benign = 0, Malignant = 1)\n",
        "\n",
        "1. Assign equal weights to all training instances.\n",
        "\n",
        "2. Train a weak learner (e.g., decision stump).\n",
        "\n",
        "3. Calculate error rate and assign a weight (alpha) to the learner based on performance.\n",
        "\n",
        "4. Increase weights for misclassified points, decrease for correctly classified ones.\n",
        "\n",
        "5. Repeat steps 2–4 for several iterations.\n",
        "\n",
        "6. Final prediction is a weighted sum of all weak learners.\n",
        "\n",
        "---\n",
        "###5. What is Gradient Boosting, and how is it different from AdaBoost?\n",
        "- Gradient Boosting builds models sequentially like AdaBoost.\n",
        "\n",
        "- Instead of focusing on misclassified instances, it minimizes a differentiable loss function (like MSE or log loss) using gradient descent.\n",
        "\n",
        "- More flexible than AdaBoost; works well for both regression and classification.\n",
        "\n",
        "---\n",
        "###6. What is the loss function in Gradient Boosting?\n",
        "Depends on the problem:\n",
        "\n",
        "- Regression: Mean Squared Error (MSE) or Mean Absolute Error (MAE)\n",
        "\n",
        "- Classification: Log Loss (Binary Cross-Entropy)\n",
        "\n",
        "The model tries to minimize the loss function using negative gradients as pseudo-residuals.\n",
        "\n",
        "---\n",
        "###7. How does XGBoost improve over traditional Gradient Boosting?\n",
        "- Regularization to prevent overfitting\n",
        "\n",
        "- Parallelized tree construction\n",
        "\n",
        "- Handling of missing values\n",
        "\n",
        "- Pruning (max_depth) instead of depth-first tree growth\n",
        "\n",
        "- Cache optimization for faster computation\n",
        "\n",
        "---\n",
        "###8. What is the difference between XGBoost and CatBoost?\n",
        "| Feature              | **XGBoost**                              | **CatBoost**                                          |\n",
        "| -------------------- | ---------------------------------------- | ----------------------------------------------------- |\n",
        "| Handles categoricals | Must be encoded manually (One-Hot/Label) | Handles categorical features **natively**             |\n",
        "| Training Speed       | Very fast                                | Slightly slower but more accurate on categorical data |\n",
        "| Overfitting          | Good regularization options              | Great default regularization and model tuning         |\n",
        "| Ease of Use          | More manual tuning needed                | Works well **out-of-the-box**                         |\n",
        "\n",
        "---\n",
        "###9. What are some real-world applications of Boosting techniques?\n",
        "- Fraud Detection – Financial institutions\n",
        "\n",
        "- Cancer Detection – Healthcare (e.g., Breast Cancer dataset)\n",
        "\n",
        "- Click-Through Rate Prediction – Ads & Recommendations\n",
        "\n",
        "- Credit Scoring – Loan approval systems\n",
        "\n",
        "- Customer Churn Prediction – Telecom and SaaS companies\n",
        "\n",
        "---\n",
        "###10. How does regularization help in XGBoost?\n",
        "Regularization in XGBoost:\n",
        "\n",
        "- Prevents overfitting by penalizing complex trees.\n",
        "\n",
        "- Uses L1 (Lasso) and L2 (Ridge) regularization terms in the loss function.\n",
        "\n",
        "- Helps the model generalize better on unseen data.\n",
        "\n",
        "---\n",
        "###11. What are some hyperparameters to tune in Gradient Boosting models?\n",
        "Key hyperparameters:\n",
        "\n",
        "- n_estimators – Number of trees\n",
        "\n",
        "- learning_rate – Shrinks the contribution of each tree\n",
        "\n",
        "- max_depth – Max depth of individual trees\n",
        "\n",
        "- min_samples_split / min_child_weight – Minimum samples to split a node\n",
        "\n",
        "- subsample – Fraction of samples used per tree\n",
        "\n",
        "- colsample_bytree – Features used per tree\n",
        "\n",
        "- loss – Loss function to minimize\n",
        "\n",
        "- alpha, lambda – Regularization terms (in XGBoost)\n",
        "\n",
        "---\n",
        "###12. What is the concept of Feature Importance in Boosting?\n",
        "- Feature importance shows how useful each feature is in building the boosted trees.\n",
        "\n",
        "- Measured by:\n",
        "\n",
        "  - Gain – Improvement brought by a feature to the branches.\n",
        "\n",
        "  - Cover – Number of samples affected by splits on a feature.\n",
        "\n",
        "  - Frequency – Number of times a feature is used in trees.\n",
        "\n",
        "Helps in:\n",
        "\n",
        "- Feature selection\n",
        "\n",
        "- Model interpretability\n",
        "\n",
        "---\n",
        "###13. Why is CatBoost efficient for categorical data?\n",
        "- Native support for categorical features using ordered boosting and target statistics.\n",
        "\n",
        "- No need for one-hot encoding or manual preprocessing.\n",
        "\n",
        "- Uses efficient encoding techniques to prevent data leakage and overfitting.\n",
        "\n",
        "- Automatically handles high-cardinality features (like zip codes, user IDs).\n",
        "\n",
        "---\n",
        "---\n",
        "#Practical Answers:\n",
        "---\n",
        "---\n",
        "###14. Train an AdaBoost Classifier on a sample dataset and print model accuracy."
      ],
      "metadata": {
        "id": "yc5Obr5xe1te"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mF4ApHwTeyTS"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost\n",
        "clf = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"AdaBoost Classifier Accuracy:\", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###15. Train an AdaBoost Regressor and evaluate using MAE"
      ],
      "metadata": {
        "id": "812yzY7hW4C5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.3, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost Regressor\n",
        "regr = AdaBoostRegressor(n_estimators=100, random_state=42)\n",
        "regr.fit(X_train, y_train)\n",
        "y_pred = regr.predict(X_test)\n",
        "\n",
        "# MAE\n",
        "print(\"AdaBoost Regressor MAE:\", mean_absolute_error(y_test, y_pred))"
      ],
      "metadata": {
        "id": "jadaoZrAW8Ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###16. Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance"
      ],
      "metadata": {
        "id": "ZH8MpZnpW_CU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train model\n",
        "model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Feature importances\n",
        "importances = model.feature_importances_\n",
        "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(importance_df)\n",
        "\n",
        "# Optional: plot\n",
        "importance_df.plot(kind='bar', x='Feature', y='Importance', figsize=(12, 5), title=\"Feature Importances\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S1hc3QNbXCiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###17. Train a Gradient Boosting Regressor and evaluate using R-Squared Score\n"
      ],
      "metadata": {
        "id": "htYgKReuXGdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Use same regression data\n",
        "regressor = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# R2 Score\n",
        "print(\"Gradient Boosting Regressor R2 Score:\", r2_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "8Pq1zVa6XN85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###18. Train an XGBoost Classifier and compare accuracy with Gradient Boosting"
      ],
      "metadata": {
        "id": "HW6xzAKDXYjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train XGBoost\n",
        "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "xgb_pred = xgb_clf.predict(X_test)\n",
        "\n",
        "# Accuracy Comparison\n",
        "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, regr.predict(X_test)))\n",
        "print(\"XGBoost Classifier Accuracy:\", accuracy_score(y_test, xgb_pred))"
      ],
      "metadata": {
        "id": "cjM8cz7cXaKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###19. Train a CatBoost Classifier and evaluate using F1-Score"
      ],
      "metadata": {
        "id": "FodFH2oZXdPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Train CatBoost\n",
        "cat_clf = CatBoostClassifier(verbose=0, random_seed=42)\n",
        "cat_clf.fit(X_train, y_train)\n",
        "cat_pred = cat_clf.predict(X_test)\n",
        "\n",
        "# F1 Score\n",
        "print(\"CatBoost Classifier F1 Score:\", f1_score(y_test, cat_pred))"
      ],
      "metadata": {
        "id": "z8vyWObgXhJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###20. Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "aVsVOxh7Xkzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "xgb_reg = XGBRegressor(n_estimators=100, random_state=42)\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "y_pred = xgb_reg.predict(X_test)\n",
        "\n",
        "print(\"XGBoost Regressor MSE:\", mean_squared_error(y_test, y_pred))"
      ],
      "metadata": {
        "id": "IjjLkRi2YQeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###21. Train an AdaBoost Classifier and visualize feature importance."
      ],
      "metadata": {
        "id": "RV5DxnJvYViW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "clf = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.bar(range(X.shape[1]), clf.feature_importances_)\n",
        "plt.xlabel(\"Feature Index\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.title(\"AdaBoost Feature Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XZVly6TPYa2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###22. Train a Gradient Boosting Regressor and plot learning curves."
      ],
      "metadata": {
        "id": "UykA3F13YkFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "train_errors = []\n",
        "test_errors = []\n",
        "\n",
        "for y_train_pred, y_test_pred in zip(regressor.staged_predict(X_train), regressor.staged_predict(X_test)):\n",
        "    train_errors.append(mean_squared_error(y_train, y_train_pred))\n",
        "    test_errors.append(mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "plt.plot(train_errors, label=\"Train\")\n",
        "plt.plot(test_errors, label=\"Test\")\n",
        "plt.legend()\n",
        "plt.title(\"Gradient Boosting Learning Curves\")\n",
        "plt.xlabel(\"Number of Estimators\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sI9uC0MMYoO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###23. Train an XGBoost Classifier and visualize feature importance."
      ],
      "metadata": {
        "id": "xyK6p_B8Yr0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import plot_importance\n",
        "\n",
        "plot_importance(xgb_clf, importance_type='gain', max_num_features=10, title=\"Top Features - XGBoost\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MXldImw_Yvkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###24. Train a CatBoost Classifier and plot the confusion matrix."
      ],
      "metadata": {
        "id": "l8TNEkYaZP-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, cat_pred)\n",
        "plt.title(\"CatBoost Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l3dt8j23ZT4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###25. Train an AdaBoost Classifier with different numbers of estimators and compare accuracy."
      ],
      "metadata": {
        "id": "Hn7OXYxmZXLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for n in [10, 50, 100, 200]:\n",
        "    model = AdaBoostClassifier(n_estimators=n, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    acc = model.score(X_test, y_test)\n",
        "    print(f\"n_estimators={n}, Accuracy={acc:.4f}\")"
      ],
      "metadata": {
        "id": "lZeJbDbJZbEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###26. Train a Gradient Boosting Classifier and visualize the ROC curve."
      ],
      "metadata": {
        "id": "AEgBqdxYZgnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import RocCurveDisplay\n",
        "\n",
        "model = GradientBoostingClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "RocCurveDisplay.from_estimator(model, X_test, y_test)\n",
        "plt.title(\"ROC Curve - Gradient Boosting\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BNTgru0dZkgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###27. Train an XGBoost Regressor and tune the learning rate using GridSearchCV."
      ],
      "metadata": {
        "id": "FvQ09YUiZngL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'learning_rate': [0.01, 0.05, 0.1, 0.2]}\n",
        "grid = GridSearchCV(XGBRegressor(n_estimators=100), param_grid, scoring='neg_mean_squared_error', cv=3)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Learning Rate:\", grid.best_params_)\n",
        "print(\"Best MSE:\", -grid.best_score_)"
      ],
      "metadata": {
        "id": "GhTmD9UPZ4GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###28. Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting."
      ],
      "metadata": {
        "id": "x3VPCx7uZsyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X_imb, y_imb = make_classification(n_samples=1000, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Without class weights\n",
        "cat1 = CatBoostClassifier(verbose=0, random_seed=42)\n",
        "cat1.fit(X_imb, y_imb)\n",
        "y_pred1 = cat1.predict(X_imb)\n",
        "print(\"Without Class Weights:\\n\", classification_report(y_imb, y_pred1))\n",
        "\n",
        "# With class weights\n",
        "cat2 = CatBoostClassifier(class_weights=[1, 10], verbose=0, random_seed=42)\n",
        "cat2.fit(X_imb, y_imb)\n",
        "y_pred2 = cat2.predict(X_imb)\n",
        "print(\"With Class Weights:\\n\", classification_report(y_imb, y_pred2))"
      ],
      "metadata": {
        "id": "w6DGoHD3Z7wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###29. Train an AdaBoost Classifier and analyze the effect of different learning rates."
      ],
      "metadata": {
        "id": "rE-G2VwRZvpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for lr in [0.01, 0.1, 0.5, 1.0]:\n",
        "    model = AdaBoostClassifier(learning_rate=lr, n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    acc = model.score(X_test, y_test)\n",
        "    print(f\"Learning Rate={lr}, Accuracy={acc:.4f}\")"
      ],
      "metadata": {
        "id": "rvp6ZvEVZ-jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###30. Train an XGBoost Classifier for multi-class classification and evaluate using log-loss."
      ],
      "metadata": {
        "id": "s9jrGX-oZzGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "digits = load_digits()\n",
        "X_m, y_m = digits.data, digits.target\n",
        "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_m, y_m, test_size=0.2, random_state=42)\n",
        "\n",
        "multi_xgb = XGBClassifier(objective='multi:softprob', num_class=10, eval_metric='mlogloss', random_state=42)\n",
        "multi_xgb.fit(X_train_m, y_train_m)\n",
        "y_proba = multi_xgb.predict_proba(X_test_m)\n",
        "\n",
        "print(\"Multi-class Log Loss:\", log_loss(y_test_m, y_proba))"
      ],
      "metadata": {
        "id": "SzJDF6PtaBVK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}